---
title: "ã¯ã˜ã‚ã¦ã®æ©Ÿæ¢°å­¦ç¿’(è‡ªåˆ†ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã‚‹ç·¨)"
emoji: "ğŸ¤–"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["Python","TensorFlow","ML","æ©Ÿæ¢°å­¦ç¿’"]
published: true
---

## ã¯ã˜ã‚ã«

æ©Ÿæ¢°å­¦ç¿’ã‚’ã‚„ã£ã¦ã¿ãŸãã¦ã€ã¨ã‚Šã‚ãˆãšã‚µãƒ³ãƒ—ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã€ä½•ã‹ã§ãã¦ã„ã‚‹ã£ã½ã„ã‘ã‚Œã©ã€ãã®å¾Œã©ã†ã—ã¦è‰¯ã„ã‹ã‚ã‹ã‚‰ãªã„ã€ãã‚“ãªäººã¯å¤šã„ã¨æ€ã„ã¾ã™ã€‚
ã“ã®è¨˜äº‹ã§ã¯ã€å…¨ãã®æ©Ÿæ¢°å­¦ç¿’åˆå¿ƒè€…å‘ã‘ã«ã€è‡ªåˆ†ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã£ã¦ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å­¦ç¿’ã•ã›ã¦ã¿ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œã£ã¦ã¿ã¾ã™ã€‚

ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã¯

[kaityo256/my_first_ml](https://github.com/kaityo256/my_first_ml)

ã«ã‚ã‚Šã¾ã™ãŒã€cloneã—ãŸã‚Šã›ãšã€ä»¥ä¸‹ã‚’æ‰‹ã§å†™ã—ãªãŒã‚‰ä½œæ¥­ã—ãŸã»ã†ãŒè‰¯ã„ã¨æ€ã„ã¾ã™ã€‚

## MNISTã®å­¦ç¿’

æ©Ÿæ¢°å­¦ç¿’ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã„ãˆã°ã€MNISTã§ã™ã€‚ã“ã‚Œã¯æ‰‹æ›¸ãæ•°å­—ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€0ã‹ã‚‰9ã¾ã§ã®æ‰‹æ›¸ãæ•°å­—ãƒ‡ãƒ¼ã‚¿ã¨ã€ãã®æ­£è§£ãƒ©ãƒ™ãƒ«ãŒã‚»ãƒƒãƒˆã«ãªã£ã¦ã„ã¾ã™ã€‚å¤šãã®æ©Ÿæ¢°å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã€MNISTã¯æ¨™æº–ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚

TensorFlowã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã€ã¾ãšã¯TensorFlowã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚Google Colabã¨ã‹ã§å®Ÿè¡Œã™ã‚‹ã®ãŒæ¥½ã§ã™ãŒã€ã‚‚ã—ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã—ãŸã„å ´åˆã¯ã€ä»Šå¾Œã®ãŸã‚ã«ä»®æƒ³ç’°å¢ƒã‚’ä½œã£ã¦ãŠãã¨è‰¯ã„ã§ã—ã‚‡ã†ã€‚é©å½“ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª(ä¾‹ãˆã°`my_first_ml`)ã‚’ä½œã£ã¦ã€ãã“ã§ä½œæ¥­ã—ã¾ã—ã‚‡ã†ã€‚

```sh
mkdir my_first_ml
cd my_first_ml
```

æ¬¡ã«ã€ä»®æƒ³ç’°å¢ƒã‚’ä½œã‚Šã¾ã™ã€‚Pythonã¯æ§˜ã€…ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ä½¿ã„ã¾ã™ãŒã€ãã‚Œã‚‰ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã¶ã¤ã‹ã£ãŸã‚Šã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒç•°ãªã‚‹ã¨ãµã‚‹ã¾ã„ãŒå¤‰ã‚ã£ãŸã‚Šã—ã¦ä¸ä¾¿ã§ã™ã€‚ã“ã‚Œã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿å…¨ä½“ã§ç®¡ç†ã™ã‚‹ã¨ã€åˆ¥ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒåˆ¥ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¨ã¶ã¤ã‹ã£ã¦ã€ã„ã¤ã®ã¾ã«ã‹å‹•ã‹ãªããªã£ã¦ã„ãŸã€ãªã‚“ã¦ã“ã¨ãŒãŠããŸã‚Šã—ã¾ã™ã€‚ãã‚Œã‚’é˜²ããŸã‚ã«ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã”ã¨ã«ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ç®¡ç†ã—ã¾ã™ã€‚ãã®ãŸã‚ã«ä½¿ã†ã®ãŒä»®æƒ³ç’°å¢ƒã§ã™ã€‚

```sh
python3 -m venv myenv
source myenv/bin/activate
```

ã“ã‚Œã«ã‚ˆã‚Šã€ä»®æƒ³ç’°å¢ƒ`myenv`ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆã•ã‚Œã¾ã—ãŸã€‚ä»¥å¾Œã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã‚‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¯ã€`my_first_ml/myenv`ä»¥ä¸‹ã«å…¥ã‚Šã¾ã™ã€‚

```sh
python3 -m pip install --upgrade pip
python3 -m pip install tensorflow
```

TensorFlowãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚ŒãŸã‹ã©ã†ã‹ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚IPythonã‚’ä½¿ã†ã®ãŒè‰¯ã„ã¨æ€ã„ã¾ã™ã€‚

```sh
$ ipython3
In [1]: import tensorflow as tf
In [2]: tf.__version__
Out[2]: '2.10.0'
In [3]: exit 
```

`tf.__version__`ã‚’è©•ä¾¡ã—ã¦ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒå¸°ã£ã¦ããŸã‚‰æ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™ã€‚`exit`ã§IPythonã‚’æŠœã‘ã¦ãŠãã¾ã—ã‚‡ã†ã€‚

ã“ã‚Œã§TensorFlowã‚’ä½¿ã†æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚å®Ÿéš›ã«æ©Ÿæ¢°å­¦ç¿’ã‚’ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

TensorFlow/Kerasã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’çµ„ã‚“ã§MNISTã‚’å­¦ç¿’ã•ã›ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã¯ã“ã‚“ãªæ„Ÿã˜ã«ãªã‚Šã¾ã™ã€‚

```py
import numpy as np
import tensorflow as tf
from tensorflow import keras


def get_data():
    train_data, test_data = keras.datasets.mnist.load_data()
    train_images, train_labels = train_data
    test_images, test_labels = test_data
    train_images = train_images / 255.0
    test_images = test_images / 255.0
    return(train_images, train_labels, test_images, test_labels)


def create_model():
    model = keras.Sequential([
        keras.layers.Flatten(input_shape=(28, 28)),
        keras.layers.Dense(128, activation='relu'),
        keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model


(train_images, train_labels, test_images, test_labels) = get_data()

model = create_model()

model.fit(train_images, train_labels, epochs=5)

test_loss, test_acc = model.evaluate(test_images, test_labels)

print(f"Test Loss = {test_loss}")
print(f"Test Accuracy = {test_acc}")
```

ãŸã£ãŸã“ã‚Œã ã‘ã§ã™ã€‚ã“ã‚Œã‚’`mnist_train.py`ã¨ã„ã†åå‰ã§ä¿å­˜ã—ã€å®Ÿè¡Œã—ã¾ã—ã‚‡ã†ã€‚

```sh
$ python3 mnist_train.py
(snip)
Epoch 1/5
1875/1875 [==============================] - 4s 2ms/step - loss: 0.2640 - accuracy: 0.9252
Epoch 2/5
1875/1875 [==============================] - 4s 2ms/step - loss: 0.1173 - accuracy: 0.9649
Epoch 3/5
1875/1875 [==============================] - 4s 2ms/step - loss: 0.0807 - accuracy: 0.9762
Epoch 4/5
1875/1875 [==============================] - 4s 2ms/step - loss: 0.0597 - accuracy: 0.9816
Epoch 5/5
1875/1875 [==============================] - 5s 2ms/step - loss: 0.0467 - accuracy: 0.9852
313/313 [==============================] - 1s 2ms/step - loss: 0.0815 - accuracy: 0.9746
Test Loss = 0.08145684003829956
Test Accuracy = 0.9746000170707703
```

æœ€åˆã«ã”ã¡ã‚ƒã”ã¡ã‚ƒè¡¨ç¤ºã•ã‚Œã‚‹ã®ã¯TensorFlowã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãŸæ™‚ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã®ã§ã€ã¨ã‚Šã‚ãˆãšç„¡è¦–ã—ã¦ã‹ã¾ã„ã¾ã›ã‚“ã€‚ä»Šå›ã¯ã‚¨ãƒãƒƒã‚¯ã‚’5ã«ã—ãŸã®ã§ã€5å›åˆ†å­¦ç¿’ã—ã€å¾ã€…ã«ç²¾åº¦ãŒä¸ŠãŒã£ã¦ã„ã‚‹ã“ã¨ã€æœ€å¾Œã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ç²¾åº¦ã‚’ç¢ºèªã—ã€ãƒ­ã‚¹ãŒ0.081ã€ç²¾åº¦ãŒ97.4%ã§ã‚ã£ãŸã“ã¨ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

## ã‚³ãƒ¼ãƒ‰ã®èª¬æ˜

ã•ã¦ã€ã‚ãšã‹æ•°åè¡Œæ›¸ã„ãŸã‚‰æ©Ÿæ¢°å­¦ç¿’ãŒã§ãã‚‹æ™‚ä»£ã«ãªã‚Šã¾ã—ãŸãŒã€ãã®åˆ†ã€å®Ÿè£…ãŒéš è”½ã•ã‚Œã¦ãŠã‚Šã€ä½•ãŒèµ·ãã¦ã„ã‚‹ã‹ã‚ã‹ã‚Šã«ãããªã£ã¦ã„ã¾ã™ã€‚å…ˆã»ã©ã®ã‚³ãƒ¼ãƒ‰ãŒä½•ã‚’ã—ã¦ã„ã‚‹ã‹ã€èª¿ã¹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
import numpy as np
import tensorflow as tf
from tensorflow import keras
```

æœ€åˆã®æ–¹ã¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ã™ã€‚`as`ã¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã«åˆ¥åã‚’ã¤ã‘ã‚‹å‘½ä»¤ã§ã€æ…£ç¿’ã¨ã—ã¦`numpy`ã¯`np`ã€`tensorflow`ã¯`tf`ã¨ç•¥ã—ã¾ã™ã€‚ä»Šå¾Œã€`tensorflow.hogehoge`ã¨æ›¸ãã‹ã‚ã‚Šã«`tf.hogehoge`ã¨æ›¸ã‘ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```py
def get_data():
    train_data, test_data = keras.datasets.mnist.load_data()
    train_images, train_labels = train_data
    test_images, test_labels = test_data
    train_images = train_images / 255.0
    test_images = test_images / 255.0
    return(train_images, train_labels, test_images, test_labels)
```

ã“ã‚Œã¯ã€MNISTã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹é–¢æ•°ã§ã™ã€‚`keras`ã«ã¯æ¨™æº–ã§ã„ãã¤ã‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒä»˜å±ã—ã¦ãŠã‚Šã€`keras.datasets.hogehoge.load_data()`ã§ãƒ‡ãƒ¼ã‚¿ã‚’æŒã£ã¦ã“ã‚Œã¾ã™ã€‚MNISTã®å ´åˆã¯ã€`keras.datasets.mnist.load_data()`ã¨ã™ã‚‹ã¨ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚¿ãƒ—ãƒ«ã§æ¸¡ã•ã‚Œã‚‹ã®ã§ã€ãã‚Œã‚’ã‚¿ãƒ—ãƒ«ã§å—ã‘å–ã‚Šã¾ã™ã€‚

å—ã‘å–ã£ãŸè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯ã€ãã‚Œãã‚Œã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã¨æ­£è§£ãƒ©ãƒ™ãƒ«ã®ã‚¿ãƒ—ãƒ«ã«ãªã£ã¦ã„ã¾ã™ã€‚ãªã®ã§ã€ãã‚Œã‚‰ã‚’ã‚¿ãƒ—ãƒ«ã¨ã—ã¦åˆ†é›¢ã—ã¾ã™ã€‚

```py
train_images, train_labels = train_data
test_images, test_labels = test_data
```

`train_images`ã‚„`test_images`ã¯ã€NumPyé…åˆ—ã«ãªã£ã¦ã„ã¾ã™ã€‚ä¾‹ãˆã°`train_images[0]`ã¨ã™ã‚‹ã¨ã€æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã§ãã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯28x28ã®NumPyé…åˆ—ã«ãªã£ã¦ãŠã‚Šã€æ–‡å­—ã®ã€Œè¼åº¦ã€ãŒ0ã‹ã‚‰255ã®æ•´æ•°ã§æ ¼ç´ã•ã‚Œã¦ã„ã¾ã™ã€‚

å¾Œã®å­¦ç¿’ã®ãŸã‚ã€ã“ã‚Œã‚’0ã‹ã‚‰1ã®å®Ÿæ•°ã«æ­£è¦åŒ–ã—ã¦ãŠãã¾ã™ã€‚ãã‚ŒãŒä»¥ä¸‹ã®è¡Œã§ã™ã€‚

```sh
train_images = train_images / 255.0
test_images = test_images / 255.0
```

æ­£è§£ãƒ©ãƒ™ãƒ«ã¯ã€NumPyã®ä¸€æ¬¡å…ƒé…åˆ—ã§ã€ãŸã¨ãˆã°`train_labels[0]`ã«ã¯ã€Œ5ã€ãŒæ ¼ç´ã•ã‚Œã¦ãŠã‚Šã€0ç•ªç›®ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã®æ­£è§£ãƒ©ãƒ™ãƒ«ãŒ5ã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

ã‚ã¨ã¯ã€æ­£è¦åŒ–ã—ãŸè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€ãã®ãƒ©ãƒ™ãƒ«ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã€ãã®ãƒ©ãƒ™ãƒ«ã‚’4ã¤ã®ã‚¿ãƒ—ãƒ«ã¨ã—ã¦è¿”ã—ã¦ã„ã¾ã™ã€‚

æ¬¡ã«ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã§ã™ã€‚

```sh
def create_model():
    model = keras.Sequential([
        keras.layers.Flatten(input_shape=(28, 28)),
        keras.layers.Dense(128, activation='relu'),
        keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model
```

ã“ã®é–¢æ•°ã§ã¯ã€28x28ã®å…¥åŠ›ã‚’å—ã‘å–ã‚Šã€10ç¨®é¡ã«åˆ†é¡ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’çµ„ã‚“ã§ã„ã¾ã™ã€‚æœ€åˆã®

```py
keras.layers.Flatten(input_shape=(28, 28)),
```

ãŒå…¥åŠ›å±¤ã§ã™ã€‚28x28ã®ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚Šã€ãã‚Œã‚’`Flatten`ã«ã‚ˆã‚Šä¸€æ¬¡å…ƒé…åˆ—å¤‰æ›ã—ã¦ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«å…¥åŠ›ã™ã‚‹ã‚ˆã€ã¨æ›¸ã„ã¦ã‚ã‚Šã¾ã™ã€‚

æ¬¡ã®è¡ŒãŒä¸­é–“å±¤ã®å®šç¾©ã§ã™ã€‚

```py
keras.layers.Dense(128, activation='relu'),
```

128å€‹ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‹ã‚‰ãªã‚‹ä¸­é–“å±¤ã§ã€æ´»æ€§åŒ–é–¢æ•°ã¨ã—ã¦ReLUã‚’ä½¿ã†ã‚ˆã€ã¨æ›¸ã„ã¦ã‚ã‚Šã¾ã™ã€‚

æœ€å¾ŒãŒå‡ºåŠ›å±¤ã§ã™ã€‚

```py
keras.layers.Dense(10, activation='softmax')
```

10å€‹ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‹ã‚‰ãªã‚‹å‡ºåŠ›å±¤ã‚’ä½œã‚‹ã‚ˆã€ã¨æ›¸ã„ã¦ã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å…¥åŠ›å±¤ã¨ä¸­é–“å±¤ã€ä¸­é–“å±¤ã¨å‡ºåŠ›å±¤ãŒãã‚Œãã‚Œå…¨çµåˆã—ãŸä¸‰å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒã§ãã¾ã™ã€‚

æœ€å¾Œã®è¡ŒãŒãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã§ã™ã€‚

```py
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
```

æœ€é©åŒ–æ‰‹æ³•ã¯Adamã€ãƒ­ã‚¹(ç›®çš„é–¢æ•°)ã¯ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€é€”ä¸­çµŒéã§`accuracy`ã‚’è¡¨ç¤ºã™ã‚‹ã‚ˆã†ã«æŒ‡å®šã—ã¦ã„ã¾ã™ã€‚æ…£ã‚Œã‚‹ã¾ã§ã¯ã“ã“ã¯å¤‰ãˆãªãã¦ã‚ˆã„ã¨æ€ã„ã¾ã™ã€‚

æœ€å¾Œã«`retrun model`ã§ã€ä½œã£ãŸãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã—ã¦ã„ã¾ã™ã€‚

ã•ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã‚‹é–¢æ•°ã¨ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚‹é–¢æ•°ã‚’ä½œã£ãŸã®ã§ã€ãã‚Œã‚‰ã‚’ä½¿ã£ã¦å­¦ç¿’ã—ã¾ã—ã‚‡ã†ã€‚ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨æ„ã—ã¦ã€`model.fit`ã¨ã„ã†é–¢æ•°ã«è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’é£Ÿã‚ã›ã‚‹ã ã‘ã§ã™ã€‚

```py
(train_images, train_labels, test_images, test_labels) = get_data()
model = create_model()
model.fit(train_images, train_labels, epochs=5)
```

`model.fit`ã®ç¬¬ä¸€å¼•æ•°ã«è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€ç¬¬äºŒå¼•æ•°ã«æ­£è§£ãƒ©ãƒ™ãƒ«ã€æœ€å¾Œã«ã‚¨ãƒãƒƒã‚¯æ•°ã‚’æŒ‡å®šã—ã¦ã„ã¾ã™ã€‚ä»–ã«ã‚‚ã„ã‚ã„ã‚æŒ‡å®šã§ãã¾ã™ãŒã€ã¾ãšã¯ã‚¨ãƒãƒƒã‚¯ã ã‘ã„ã˜ã‚‹ã®ãŒè‰¯ã„ã¨æ€ã„ã¾ã™ã€‚

ã“ã‚Œã§ã€`train_images`ã‚’å—ã‘å–ã‚Šã€ãã®æ­£è§£ãƒ©ãƒ™ãƒ«ã§ã‚ã‚‹`train_labels`ã«å¯¾å¿œã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®é‡ã¿ãŒä¸€ç•ªå¤§ãããªã‚‹ã‚ˆã†ã«å­¦ç¿’ãŒé€²ã¿ã¾ã™ã€‚

å­¦ç¿’ãŒæ¸ˆã‚“ã ã‚‰ã€ã€Œå­¦ç¿’ã«ä½¿ã£ã¦ã„ãªã„ãƒ‡ãƒ¼ã‚¿ã€ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼ã‚’è¡Œã„ã¾ã—ã‚‡ã†ã€‚å…ˆç¨‹åˆ†ã‘ã¦ãŠã„ãŸãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã„ã¾ã™ã€‚

```py
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"Test Loss = {test_loss}")
print(f"Test Accuracy = {test_acc}")
```

`model.evaluate`ã®ç¬¬ä¸€å¼•æ•°ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã€ç¬¬äºŒå¼•æ•°ã«æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’æ¸¡ã™ã¨ã€ã“ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã†ã¡ã€ä½•å€‹æ­£è§£ã§ããŸã‹ã‚’è¿”ã—ã¦ãã‚Œã¾ã™ã€‚ã¾ãŸã€ãƒ­ã‚¹ã¨ã—ã¦ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚‚è¨ˆç®—ã—ã¦ãã‚Œã¾ã™ã€‚ãã‚Œã‚‰ã‚’`print`ã§è¡¨ç¤ºã—ã¦ãŠã—ã¾ã„ã§ã™ã€‚97ï½98%ç¨‹åº¦ã®æ­£ç­”ç‡ãŒå‡ºã›ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã¨æ€ã„ã¾ã™ã€‚

## ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã¨èª­ã¿è¾¼ã¿

ã›ã£ã‹ãå­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ãªã®ã§ã€ã‚ã¨ã§ä½¿ã„ãŸã„ã§ã™ã‚ˆã­ã€‚ãã®ãŸã‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

å…ˆç¨‹ä½œæˆã—ãŸã‚³ãƒ¼ãƒ‰`mnist_train.py`ã®æœ€å¾Œã«ä¸€è¡Œä»˜ã‘åŠ ãˆã‚‹ã ã‘ã§ã™ã€‚

```py
model.save_weights('model')
```

```sh
python3 mnist_train.py
```

ã¨ã—ã¦å®Ÿè¡Œã™ã‚‹ã¨ã€ã¾ãŸå­¦ç¿’ã‚’ã—ã¦ã€æœ€å¾Œã«ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯`model.data-00000-of-00001`ã¨`model.index`ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã‚’èª­ã¿è¾¼ã‚“ã§ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’é£Ÿã‚ã›ã¦çµæœã‚’è¡¨ç¤ºã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ä»¥ä¸‹ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ã‚’`mnist_load.py`ã¨ã—ã¦ä½œæˆã—ã¾ã™ã€‚

```py
import numpy as np
import tensorflow as tf
from tensorflow import keras


def get_data():
    train_data, test_data = keras.datasets.mnist.load_data()
    train_images, train_labels = train_data
    test_images, test_labels = test_data
    train_images = train_images / 255.0
    test_images = test_images / 255.0
    return(train_images, train_labels, test_images, test_labels)


def create_model():
    model = keras.Sequential([
        keras.layers.Flatten(input_shape=(28, 28)),
        keras.layers.Dense(128, activation='relu'),
        keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model


(train_images, train_labels, test_images, test_labels) = get_data()

model = create_model()
model.load_weights('model')

predictions = model.predict(test_images[0:20])

for i in range(5):
    predicted_index = np.argmax(predictions[i])
    print(f"prediction= {predicted_index} answer = {test_labels[i]}")
```

é€”ä¸­ã¾ã§`mnist_train.py`ã¨ã»ã¨ã‚“ã©åŒã˜ã§ã™ã€‚æ…£ã‚ŒãŸã‚‰å…±é€šéƒ¨åˆ†ã‚’åˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¾ã¨ã‚ã‚‹ã¨è‰¯ã„ã§ã—ã‚‡ã†ã€‚ç•°ãªã‚‹ã®ã¯ã€`model.fit`ã§é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ãŸã¨ã“ã‚ã‚’ã€`mode.load_weights`ã§é‡ã¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã‚‹ã¨ã“ã‚ã§ã™ã€‚TensorFlowã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ã€Œå½¢ã€ã¯ä¿å­˜ã—ã¦ãã‚Œãªã„ã®ã§ã€ã“ã®ã‚ˆã†ã«`create_model`é–¢æ•°ã¯ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã€èª­ã¿è¾¼ã¿ã®ä¸¡æ–¹ã§å¿…è¦ã§ã™ã€‚ã‚³ãƒ¼ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã®å½¢ã ã‘ä½œã£ã¦ã€ãã®é‡ã¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã™ã€‚

ã•ã¦ã€é‡ã¿ã‚’èª­ã¿è¾¼ã‚“ã ãƒ¢ãƒ‡ãƒ«ãŒã€Œå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã€ã«ãªã‚‹ãŸã‚ã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›ã—ãŸã‚‰ã€ãã‚ŒãŒã©ã®æ‰‹æ›¸ãæ•°å­—ã§ã‚ã‚‹ã‹ã‚’äºˆæ¸¬ã—ã¦ãã‚Œã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚ã¨ã‚Šã‚ãˆãš`test_images`ã®å…ˆé ­ã®20å€‹ã‚’é£Ÿã‚ã›ã‚‹ã“ã¨ã«ã—ã¾ã—ã‚‡ã†ã€‚ã‚³ãƒ¼ãƒ‰ã®ã“ã®éƒ¨åˆ†ã§ã™ã€‚

```py
predictions = model.predict(test_images[0:20])
```

ãƒ¢ãƒ‡ãƒ«ã‚’åˆ†é¡å™¨ã‚’ã¨ã—ã¦ä½¿ã†å ´åˆã¯ã€`model.predict`ã«é…åˆ—ã‚’æ¸¡ã—ã¾ã™ã€‚ã“ã“ã§æ³¨æ„ã§ã™ãŒã€åŠ¹ç‡ã®ãŸã‚ã«ç”»åƒã‚’ã¾ã¨ã‚ã¦æ¸¡ã™ã“ã¨ãŒå‰æã«ãªã£ã¦ã„ã¾ã™ã€‚ã¤ã¾ã‚Šã€ç”»åƒã‚’ã¾ã¨ã‚ã¦æ¸¡ã™ã¨ã€ãã‚Œã‚‰ã«å¯¾ã™ã‚‹çµæœã‚’ã¾ã¨ã‚ã¦è¿”ã™ã€ã¨ã„ã†å½¢ã«ãªã£ã¦ã„ã¾ã™ã€‚ã“ã“ã§ã¯20æšã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã—ãŸã®ã§ã€20å€‹åˆ†ã®çµæœãŒ`predictions`ã¨ã—ã¦å¸°ã£ã¦ãã¾ã™ã€‚

ã•ã¦ã€`predictions`ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç”Ÿã®å‡ºåŠ›ã«ãªã£ã¦ã„ã¾ã™ã€‚20æšã®ãƒ‡ãƒ¼ã‚¿ã‚’é£Ÿã‚ã›ãŸã®ã§ã€`predictions`ã¯20æ¬¡å…ƒã®é…åˆ—ã§ã™ãŒã€ãã®é…åˆ—ã®è¦ç´ `predictions[i]`ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒ10å€‹ã®åˆ†é¡å™¨ã§ã‚ã‚‹ã“ã¨ã‚’åæ˜ ã—ã¦ã€10æ¬¡å…ƒã®é…åˆ—ã«ãªã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€æœ€å¾Œã®10å€‹ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å‡ºåŠ›ã§ã™ã€‚ãã“ã§ã€10æ¬¡å…ƒé…åˆ—`predictions[i]`ã®ã†ã¡ã€æœ€å¤§ã®å€¤ã‚’æŒã¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’`numpy.argmax`ã§æ¢ã—ã¦ã‚„ã‚Šã¾ã—ã‚‡ã†ã€‚ã“ã®ã†ã¡ã€ä¸€ç•ªå¤§ããªå‡ºåŠ›ã‚’å‡ºã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã€ã“ã®ãƒ¢ãƒ‡ãƒ«ãŒäºˆæ¸¬ã™ã‚‹çµæœã¨ãªã‚Šã¾ã™ã€‚

å®Ÿè¡Œçµæœã¯ã“ã‚“ãªæ„Ÿã˜ã«ãªã‚Šã¾ã™ã€‚

```sh
$ python3 mnist_load.py
prediction= 7 answer = 7
prediction= 2 answer = 2
prediction= 1 answer = 1
prediction= 0 answer = 0
prediction= 4 answer = 4
prediction= 1 answer = 1
prediction= 4 answer = 4
prediction= 9 answer = 9
prediction= 6 answer = 5  # â† é–“é•ãˆãŸ
prediction= 9 answer = 9
prediction= 0 answer = 0
prediction= 6 answer = 6
prediction= 9 answer = 9
prediction= 0 answer = 0
prediction= 1 answer = 1
prediction= 5 answer = 5
prediction= 9 answer = 9
prediction= 7 answer = 7
prediction= 3 answer = 3
prediction= 4 answer = 4
```

é€”ä¸­ã§6ã¨5ã‚’ï¼‘ã¤é–“é•ãˆãŸã‚ˆã†ã§ã™ã­ã€‚æ­£ç­”ç‡ãŒ97ï½98%ãªã®ã§ã€æ•°åå€‹ã«ï¼‘ã¤ã¯é–“é•ãˆã¾ã™ã€‚

## è‡ªåˆ†ã§ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã£ã¦å­¦ç¿’ã•ã›ã¦ã¿ã‚‹

### ãƒ‘ã‚¤ã“ã­å¤‰æ›

æ©Ÿæ¢°å­¦ç¿’ã§ã‚‚ã£ã¨ã‚‚é‡è¦ãªã®ã¯ã€è³ªã®è‰¯ã„ãƒ‡ãƒ¼ã‚¿ã‚’å¤§é‡ã«ç”¨æ„ã™ã‚‹ã“ã¨ã§ã™ã€‚ä¸€èˆ¬ã«ã€è‰¯ã„ãƒ‡ãƒ¼ã‚¿ã‚’å¤§é‡ã«ç”¨æ„ã™ã‚‹ã“ã¨ã¯é›£ã—ã„ã§ã™ãŒã€æ•°å€¤è¨ˆç®—ã§ã‚ã‚Œã°ã„ãã‚‰ã§ã‚‚ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã‚ˆãã‚ã‚‹ã®ã¯ã‚¤ã‚¸ãƒ³ã‚°æ¨¡å‹ã®ã‚¹ãƒ”ãƒ³ã‚³ãƒ³ãƒ•ã‚£ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ç›¸ã‚„æ¸©åº¦ã‚’æ¨å®šã•ã›ã‚‹ã€ã¨ã„ã£ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚ã“ã“ã§ã¯ã€ä¹±æ•°ã¨ãƒ‘ã‚¤ã“ã­å¤‰æ›ã‚’è¦‹åˆ†ã‘ã•ã›ã‚‹åˆ†é¡å™¨ã‚’ä½œã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

ã¾ãšã€æŒ‡å®šã®é•·ã•ã®ãƒ©ãƒ³ãƒ€ãƒ ãªæ•°åˆ—ã‚’ä½œã‚‹é–¢æ•°ã§ã™ã€‚

```py
def get_random(length):
    return np.random.random(length)
```

`numpy.random.random`ã‚’å‘¼ã¶ã ã‘ãªã®ã§ç°¡å˜ã§ã™ã­ã€‚

æ¬¡ã«ã€åˆæœŸå€¤ã‚’ä¹±æ•°ã§ä½œã‚Šã€3å€ã—ã¦ã¯æ•´æ•°éƒ¨åˆ†ã‚’å¼•ãã€ã¨ã„ã†ã“ã¨ã‚’ç¹°ã‚Šè¿”ã—ã¦ä½œã‚‹æ•°åˆ—ã‚’è¿”ã™é–¢æ•°ã§ã™ã€‚

```py
def get_baker(length):
    a = np.zeros(length)
    x = np.random.random()
    for i in range(length):
        x = x * 3.0
        x = x - int(x) 
        a[i] = x
    return a
```

ã“ã‚Œã‚‚é›£ã—ã„ã“ã¨ã¯ãªã„ã¨æ€ã„ã¾ã™ã€‚ã“ã®ã©ã¡ã‚‰ã‚‚ã€ä¸€è¦‹ä¹±æ•°ã®ã‚ˆã†ã«è¦‹ãˆã¾ã™ã€‚

ã“ã¡ã‚‰ãŒãƒ©ãƒ³ãƒ€ãƒ ãªæ•°åˆ—ã§ã™ã€‚

```py
import matplotlib.pyplot as plt
plt.plot(get_random(100),marker='.',linestyle='None')
plt.show()
```

![random](/images/my_first_machine_learning/random.png)

ã“ã¡ã‚‰ãŒãƒ‘ã‚¤ã“ã­å¤‰æ›ã§ä½œã£ãŸæ•°åˆ—ã§ã™ã€‚

```py
plt.plot(get_baker(100),marker='.',linestyle='None')
plt.show()
```

![baker](/images/my_first_machine_learning/baker.png)

ã±ã£ã¨è¦‹ã§ã¯åŒºåˆ¥ãŒã¤ãã¾ã›ã‚“ã€‚ã¾ãŸã€å¹³å‡ã‚„åˆ†æ•£ã¨ã„ã£ãŸçµ±è¨ˆé‡ã§ã‚‚åŒºåˆ¥ãŒã¤ãã¾ã›ã‚“ã€‚

```py
r = get_random(1000)
print(f"average = {np.average(r)}") #=> average = 0.4961720294078324
print(f"variance = {np.var(r)}")    #=> variance = 0.08763440008850572
```

```py
b = get_baker(1000)
print(f"average = {np.average(b)}") #=> average = 0.4938802585263077
print(f"variance = {np.var(b)}")    #=> variance = 0.08359332179469303
```

ã—ã‹ã—ã€`(data[i], data[i+1])`ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã¨ã€é•ã„ãŒè¦‹ãˆã¾ã™ã€‚

```py
def get_xy(data):
    n = len(data) -1
    x = np.zeros(n)
    y = np.zeros(n)
    for i in range(n):
        x[i] = data[i]
        y[i] = data[i+1]
    return x,y
```

ä¹±æ•°ã®å ´åˆã«ã¯æ§‹é€ ã¯ã¾ã£ãŸãè¦‹ãˆã¾ã›ã‚“ã€‚

```py
x, y = get_xy(get_random(1000))
plt.scatter(x, y, marker='.')
plt.show()
```

![random_xy](/images/my_first_machine_learning/random_xy.png)

ãƒ‘ã‚¤ã“ã­å¤‰æ›ã®å ´åˆã¯ãã‚Œã„ãªæ§‹é€ ãŒè¦‹ãˆã¾ã™ã€‚

```py
x, y = get_xy(get_baker(1000))
plt.scatter(x, y, marker='.')
plt.show()
```

![baker_xy](/images/my_first_machine_learning/baker_xy.png)

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã“ã‚Œã‚’é£Ÿã‚ã›ã¦ã€å­¦ç¿’ã«ã‚ˆã‚ŠåŒºåˆ¥ã§ãã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚

### è¨“ç·´ã‚³ãƒ¼ãƒ‰

ã¾ãšã€`mnist_train.py`ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã€`baker_train.py`ã¨ã„ã†åå‰ã§ä¿å­˜ã—ã¾ã—ã‚‡ã†ã€‚ã“ã‚Œã‚’æ”¹é€ ã™ã‚‹ã“ã¨ã§è‡ªåˆ†ã®ä½œã£ãŸãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ã•ã›ã‚‹ã‚³ãƒ¼ãƒ‰ã«ã—ã¾ã™ã€‚

æœ€åˆã«ã€é£Ÿã‚ã›ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯100å€‹ã®ä¸€æ¬¡å…ƒé…åˆ—ã€åˆ†é¡ã¯ã€Œãƒ©ãƒ³ãƒ€ãƒ ã€ã‹ã€Œãƒ‘ã‚¤ã“ã­å¤‰æ›ã€ã®äºŒç¨®é¡ãªã®ã§ã€å‡ºåŠ›ã¯2å€‹ã§ã™ã€‚ã¾ãŸã€æœ€åˆã‹ã‚‰ä¸€æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’é£Ÿã‚ã›ã‚‹ã®ã§ã€`Flatten`ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ä¸­é–“å±¤ã®æ•°ã¯ã€ã¨ã‚Šãˆãšé©å½“ã«50å€‹ãã‚‰ã„ã«ã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚`create_model`é–¢æ•°ã¯ã“ã†ãªã‚Šã¾ã™ã€‚

```py
def create_model():
    model = keras.Sequential([
        keras.layers.Dense(100), # â†ã“ã“ã‚’Flattenã‹ã‚‰Denseã«ä¿®æ­£ã€‚
        keras.layers.Dense(32, activation='relu'),
        keras.layers.Dense(2, activation='softmax') # â†ã“ã“ã‚’10ã‹ã‚‰2ã«ä¿®æ­£
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model
```

æ¬¡ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã‚‹ã¨ã“ã‚ã§ã™ã€‚`mnist_train.py`ã®`get_data`é–¢æ•°ã®ä»£ã‚ã‚Šã«ã€ä»¥ä¸‹ã‚’å…¥åŠ›ã—ã¾ã—ã‚‡ã†ã€‚

```py
def get_random(length):
    return np.random.random(length)


def get_baker(length):
    a = np.zeros(length)
    x = np.random.random()
    for i in range(length):
        x = x * 3.0
        x = x - int(x)
        a[i] = x
    return a


def make_data(n, length):
    x = []
    y = []
    for _ in range(length):
        if(np.random.random() < 0.5):
            x.append(get_random(n))
            y.append(0)
        else:
            x.append(get_baker(n))
            y.append(1)
    x = np.array(x)
    y = np.array(y)
    return x, y
```

`make_data`ã¯ã€é•·ã•`n`ã®ãƒ‡ãƒ¼ã‚¿ã‚’`length`å€‹ä½œæˆã—ã€ãƒ‡ãƒ¼ã‚¿ã¨æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’è¿”ã™é–¢æ•°ã§ã™ã€‚ã¨ã‚Šã‚ãˆãšé•·ã•ã¯100ã¨ã—ã¾ã—ã‚‡ã†ã€‚å­¦ç¿’éƒ¨åˆ†ã¯ã“ã†ãªã‚Šã¾ã™ã€‚

```py
n = 100
train_data, train_labels = make_data(n, 60000)
test_data, test_labels = make_data(n, 10000)

model = create_model()

model.fit(train_data, train_labels, epochs=5)

test_loss, test_acc = model.evaluate(test_data, test_labels)

print(f"Test Loss = {test_loss}")
print(f"Test Accuracy = {test_acc}")
```

ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã®ã¨ã“ã‚ã¯ã€åå‰ã‚’å¤‰ãˆã¦ãŠãã¾ã—ã‚‡ã†ã€‚

```py
model.save_weights('baker')
```

ä»¥ä¸Šã§æº–å‚™ã¯å®Œäº†ã§ã™ã€‚å®Ÿè¡Œã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```sh
$ python3 baker_train.py
Epoch 1/5
1875/1875 [==============================] - 3s 2ms/step - loss: 0.6435 - accuracy: 0.6104
Epoch 2/5
1875/1875 [==============================] - 3s 2ms/step - loss: 0.4758 - accuracy: 0.7699
Epoch 3/5
1875/1875 [==============================] - 3s 2ms/step - loss: 0.4047 - accuracy: 0.8117
Epoch 4/5
1875/1875 [==============================] - 3s 2ms/step - loss: 0.3718 - accuracy: 0.8306
Epoch 5/5
1875/1875 [==============================] - 3s 2ms/step - loss: 0.3527 - accuracy: 0.8410
313/313 [==============================] - 1s 1ms/step - loss: 0.3988 - accuracy: 0.8169
Test Loss = 0.3987952172756195
Test Accuracy = 0.8169000148773193
```

8å‰²ã¡ã‚‡ã£ã¨ã®æ­£ç­”ç‡ãŒå‡ºã¦ã„ã‚‹ã‚ˆã†ã§ã™ã­ã€‚

### ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿

ã›ã£ã‹ãå­¦ç¿’ã—ãŸã®ã§ã€ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ç·´ç¿’ã‚’å…¼ã­ã¦ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ãŒä½•ã‚’ã©ã®ã‚ˆã†ã«åˆ¤æ–­ã—ã¦ã„ã‚‹ã‹ã€ã¡ã‚‡ã£ã¨ã ã‘è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã¾ãšã¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ãã¾ã™ã€‚ã»ã¨ã‚“ã©`mnist_load.py`ã¨åŒã˜ã§ã™ã€‚

```py
import numpy as np
import tensorflow as tf
from tensorflow import keras


def get_random(length):
    return np.random.random(length)


def get_baker(length):
    a = np.zeros(length)
    x = np.random.random()
    for i in range(length):
        x = x * 3.0
        x = x - int(x)
        a[i] = x
    return a


def make_data(n, length):
    x = []
    y = []
    for _ in range(length):
        if(np.random.random() < 0.5):
            x.append(get_random(n))
            y.append(0)
        else:
            x.append(get_baker(n))
            y.append(1)
    x = np.array(x)
    y = np.array(y)
    return x, y


def create_model():
    model = keras.Sequential([
        keras.layers.Dense(100),
        keras.layers.Dense(32, activation='relu'),
        keras.layers.Dense(2, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model


model = create_model()
model.load_weights('baker')
```

ã“ã‚Œã§å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸã€‚ã¡ã‚ƒã‚“ã¨å­¦ç¿’ã§ãã¦ã„ã‚‹ã‹èª¿ã¹ã‚‹ãŸã‚ã€ã€Œå…¨éƒ¨ãŒä¹±æ•°ã€ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã£ã¦`model.evaluate`ã«é£Ÿã‚ã›ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
all_random_data = np.array([get_random(100) for _ in range(100)])
all_random_labels = np.array([0] * 100)

r_loss, r_acc = model.evaluate(all_random_data, all_random_labels)

print("When everything is random")
print(f"Test Loss = {r_loss}")
print(f"Test Accuracy = {r_acc}")
```

å…¨éƒ¨ãŒãƒ©ãƒ³ãƒ€ãƒ ãªã®ã§ã€æ­£è§£ãƒ©ãƒ™ãƒ«ã¯å…¨ã¦ã‚¼ãƒ­(`np.array([0] * 100)`)ã§ã™ã€‚å®Ÿè¡Œã—ã¦ã¿ã‚‹ã¨ã“ã†ãªã‚Šã¾ã™ã€‚

```sh
$ python3 baker_load.py
When everything is random
Test Loss = 0.297588586807251
Test Accuracy = 0.8500000238418579
```

85%ã®æ­£è§£ç‡ã‚’å‡ºã—ã¦ã„ã¾ã™ã€‚

é€†ã«ã€å…¨ã¦ãŒãƒ‘ã‚¤ã“ã­å¤‰æ›ã®å ´åˆã‚‚è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```py
all_baker_data = np.array([get_baker(100) for _ in range(100)])
all_baker_labels = np.array([1] * 100)

b_loss, b_acc = model.evaluate(all_baker_data, all_baker_labels)

print("When everything is baker map")
print(f"Test Loss = {b_loss}")
print(f"Test Accuracy = {b_acc}")
```

å®Ÿè¡Œã™ã‚‹ã¨ã“ã†ãªã‚Šã¾ã™ã€‚

```sh
$ python3 baker_load.py
When everything is baker map
Test Loss = 0.37267830967903137
Test Accuracy = 0.8299999833106995
```

æ­£è§£ç‡ã¯83%ã§ã™ã€‚ã“ã†ã„ã†äºŒå€¤åˆ†é¡ã§ã¯ã€ãŸã¾ã«ã€Œç‰‡æ–¹ã¯å®Œç’§ã«è­˜åˆ¥ã§ãã‚‹ãŒã€ã‚‚ã†ç‰‡æ–¹ã¯ã‚ã‹ã‚‰ãªã„ã®ã§ãƒ©ãƒ³ãƒ€ãƒ ã«ç­”ãˆã‚‹ã€ã‚ˆã†ãªåã£ãŸãƒ¢ãƒ‡ãƒ«ãŒã§ãã‚‹ã“ã¨ãŒã‚ã‚Šã€ã“ã†ã„ã†ãƒ†ã‚¹ãƒˆã‚’ã™ã‚‹ã¨ç‰‡æ–¹ã§æ­£è§£ç‡100%è¿‘ãã€ã‚‚ã†ç‰‡æ–¹ã¯50%å‰å¾Œã«ãªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ãŒã€ä»Šå›ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã¡ã‚ƒã‚“ã¨è­˜åˆ¥ã§ãã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚

## ã¾ã¨ã‚

ã¾ãšã¯MNISTã®å­¦ç¿’ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚’ç¢ºèªã—ã€ãã‚Œã‚’æ”¹é€ ã—ã¦ãƒ‘ã‚¤ã“ã­å¤‰æ›ã®ãƒ‡ãƒ¼ã‚¿ã‚’é£Ÿã‚ã›ã¦å­¦ç¿’ã•ã›ã¦è¦‹ã¾ã—ãŸã€‚æ©Ÿæ¢°å­¦ç¿’ã€ç‰¹ã«åˆ†é¡å™¨ã®ä½œæˆã¯ã€æ…£ã‚Œã¦ã—ã¾ãˆã°ãƒ‡ãƒ¼ã‚¿ã¨æ­£è§£ãƒ©ãƒ™ãƒ«ã‚’NumPyé…åˆ—ã§ä½œã‚‹ã ã‘ãªã®ã§ã™ãŒã€æœ€åˆã¯ã©ã†ã—ã¦è‰¯ã„ã‹ã‚ã‹ã‚‰ãªã„ã“ã¨ãŒå¤šã„ã®ã§ã€ã“ã†ã—ã¦ã‚³ãƒ¼ãƒ‰ã‚’æ”¹é€ ã—ã¤ã¤ã€å‹•ä½œã‚’ç¢ºèªã—ãªãŒã‚‰å®Ÿè¡Œã—ã¦ã¿ã‚‹ã¨è‰¯ã„ã¨æ€ã„ã¾ã™ã€‚

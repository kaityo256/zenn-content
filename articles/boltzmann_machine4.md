---
title: "制限ボルツマンマシンの基礎 ～微分編～"
emoji: "🤖"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["Python","ML","BM","RBM"]
published: false
---

## はじめに

機械学習で用いられるボルツマンマシン、特に制限ボルツマンマシン(Restricted Boltzmann Machine, RBM)の解説その3です。[その2](https://zenn.dev/kaityo256/articles/boltzmann_machine2)の続きなので、そちらを見てから読んでください。

## 前回までのあらすじ

ぼっち飯のDaveは、いつも学食前のテラスでお弁当を食べていますが、同じクラスのAliceとBobが学食をよく利用していることに気づきます。しかし、AliceとBobは同時に現れることは少ないようです。AliceとBobは仲が悪いのでしょうか？

Daveは何日も二人を観測し、以下のようなデータを得ました。

そこでDaveは239日にわたって二人を観察し、AliceとBobが学食に来た日、来ていない日の統計を取りました。

|  A  |  B  | 日数|
| ---- | ---- |---|
|  x  | x  | 35|
|  x  |  o  |56|
|  o  |  x  |113|
|  o  |  o  |35|

Daveはさらに、この二人が「名前は知っているくらいで、特にお互い好きでも嫌いでもない」という情報から、「学食になんらかの隠れ変数があり、その変数を通じて二人が間接的に相互作用している」というモデルを作ります。Daveからは学食で何が起きているかわかりませんが、それでもパラメータを調節して、観測事実を再現できる確率モデルを構築しました。Daveが作ったモデルは、こんなエネルギーで表現されるモデルです。

$$
H(v_a, v_b, h_c) = -(b_a v_a+ b_b v_b+ b_c h_c + W_{ac}v_a h_c + W_{bc}v_b h_c)
$$

$v_a$、$v_b$は、それぞれAliceとBobが学食に来たか来なかったかを表す変数です。0か1の値を取り、学食に来たら1、そうでなければ0で表現します。これらはDaveから見える(visible)なので、可視変数と呼びます。

$h_c$は、学食で起きている「何か」を表す変数です。それが起きていたら1、起きていなければ0をとなります。実際にAliceとBobの行動に影響を与えていたのは「カレーフェア」でしたが、それをDaveは知りません。Daveからは隠れている(hidden)ので、隠れ変数と呼びます。

$b_a, b_b, b_c$はバイアスと呼ばれる量で、対応する変数の「1になりやすさ」を表しています。例えば$b_a$の値が大きいと、Aliceが学食に来る確率が高くなります。

$W_{ac}, W_{bc}$は重みと呼ばれる量で、対応する二つの変数の「同時に1になりやすさ」を表しています。例えば$W_{ac}$は「カレーフェア」が開催されているときにAliceが学食に来たがるかどうかを表現しています。正なら来る可能性が高くなり、負なら来る可能性が低くなります。添え字はどの変数とどの変数をつないでいるかを表現しており、これが「可視変数」と「隠れ変数」しかつないでいないモデルを制限ボルツマンマシン(Restricted Boltzmann Machine, RBM)と呼びます。

このモデルでは、三つの変数$(v_a, v_b, h_c)$で状態が指定できます。すると、その状態に対応するエネルギーが$H(v_a, v_b, h_c)$で与えられ、その状態の実現確率が$\exp(-H)$に比例すると考えるのがRBMによるモデリングです。

さて、変数は「ある事実が観測されたかどうか(例えばAliceが学食に現れたかどうか)」を表します。RBMでエネルギーを定義すると、ある事実が観測される確率を求めることができます。それだけではなく、例えばAliceが学食に来てBobが来ない確率や、二人とも学食に来る確率なども全て求めることができます。このように、観測事実をモデル化し、知りたい確率を計算できるモデルを **生成モデル(Generative model)** と呼びます。RBMは生成モデルになっています。RBMは、観測事実を再現するように学習させるため、教師あり学習となります。この学習をどのように実現するのかを説明するのが本稿の目的です。

## コスト関数

一般に、機械学習ではコスト関数を定義します。コスト関数とは、これが小さくなればなるほど、モデルが目的の性質を持っていると判断できるものです。例えば手書き数字の分類などでは、誤答率をコスト関数として、それをなるべく下げるように学習を進めます。

さて、RBMの学習の目的は、何かしらの観測事実が与えられた時、それを最もよく説明できるようにパラメータを決めてやることです。Daveにとっての観測事実とは、AliceとBobが学食に来る、同時確率分布のことです。

|  A  |  B  | 日数|
| ---- | ---- |---|
|  x  | x  | 35|
|  x  |  o  |56|
|  o  |  x  |113|
|  o  |  o  |35|

例えば、Aliceが学食に来るという事象は$v_a = 1$で、来なかったという事象は$v_a=0$で表現できます。Bobも同様です。上記の表はサンプリングによるものですが、これを観測事実とすると、

$$
\begin{aligned}
P(v_a = 0, v_b = 0) &= \frac{35}{239} \\
P(v_a = 0, v_b = 1) &= \frac{56}{239} \\
P(v_a = 1, v_b = 0) &= \frac{113}{239} \\
P(v_a = 1, v_b = 1) &= \frac{35}{239}
\end{aligned}
$$

となります。この確率分布を再現するようにモデルパラメータを決めるのが目的です。後のために、それぞれの事象に通し番号をつけましょう。例えば、二人とも学食にこないという事象を1番、Bobだけ学食に来た、という事象は2番という具合です。また、いちいち$P(v_a = 0, v_b = 0)$と書くのも面倒なので、これを$q_1$と表現しましょう。同様に他の事象にも $q_2, q_3$と名前をつけます。対応は以下の通りです。

$$
\begin{aligned}
q_1 &= \frac{35}{239} \\
q_2 &= \frac{56}{239} \\
q_3 &= \frac{113}{239} \\
q_4 &= \frac{35}{239}
\end{aligned}
$$

さて、現在のパラメータにおいてRBMが事象$i$が起きると予想する確率を$p_i$としましょう。例えば事象1番、二人とも学食にこないという確率は

$$
\begin{aligned}
p_0 &= \sum_{h_c=0,1} \frac{\exp\left[H(v_a=1, v_b=1, h_c)\right]}{Z}\\
\end{aligned}
$$

で書けます。ただし$Z$は分配関数で、隠れ層も含め、全ての状態についてボルツマン重みの和をとったものです。

$$
Z = \sum_{v_a=0,1}\sum_{v_b=0,1}\sum_{h_c=0,1} \exp{\left[H(v_a=1, v_b=1, h_c)\right]}
$$